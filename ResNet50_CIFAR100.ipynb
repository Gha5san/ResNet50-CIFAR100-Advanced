{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna optuna-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adyxrQYQQt6G",
        "outputId": "94a216af-e0dc-40f4-d90b-0ec0d4c30c80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna-integration\n",
            "  Downloading optuna_integration-3.6.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna, optuna-integration\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1 optuna-integration-3.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Activation, Multiply, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers.legacy import SGD, Adam, RMSprop\n",
        "import optuna\n",
        "import math\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "WEryfIS_V8eU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (input_X_train, output_y_train), (input_X_test, output_y_test) = cifar100.load_data()\n",
        "# input_X_train = input_X_train.astype('float32') / 255\n",
        "# input_X_test = input_X_test.astype('float32') / 255\n",
        "# input_X_train = preprocess_input(input_X_train)\n",
        "# input_X_test = preprocess_input(input_X_test)\n",
        "\n",
        "# output_Y_train = to_categorical(output_y_train, 100)\n",
        "# output_Y_test = to_categorical(output_y_test, 100)"
      ],
      "metadata": {
        "id": "CjBfmpPwV9xw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(input_X_train, output_y_train), (input_X_test, output_y_test) = cifar100.load_data()\n",
        "\n",
        "# Preprocess the input data\n",
        "input_X_train = preprocess_input(input_X_train.astype('float32'))\n",
        "input_X_test = preprocess_input(input_X_test.astype('float32'))\n",
        "\n",
        "# Splitting the original training data to train and validation sets\n",
        "input_X_train, input_X_val, output_y_train, output_y_val = train_test_split(\n",
        "    input_X_train, output_y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "output_Y_train = to_categorical(output_y_train, 100)\n",
        "output_Y_test = to_categorical(output_y_test, 100)\n",
        "output_Y_val = to_categorical(output_y_val, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPN4Ai-0_g5H",
        "outputId": "58250f9e-4df3-46df-ba08-d5a85bc5bc49"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the attention mechanism\n",
        "def attach_attention_module(net, attention_module='se_block'):\n",
        "    if attention_module == 'se_block':\n",
        "        net = squeeze_excite_block(net)\n",
        "    return net\n",
        "\n",
        "def squeeze_excite_block(input, ratio=16):\n",
        "    init = input\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "# Create the model\n",
        "def create_model(trial):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    base_model.trainable = True\n",
        "\n",
        "    x = base_model.output\n",
        "    x = attach_attention_module(x)  # Attach attention module\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(trial.suggest_float('dropout_rate', 0.3, 0.5))(x)\n",
        "    predictions = Dense(100, activation='softmax')(x)\n",
        "\n",
        "    # optimizer = SGD(lr=0.1, momentum=0.9, decay=0.01)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "RJqhRsHDV_Dr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = 0  # Initial best accuracy\n",
        "best_history = None  # Variable to store the best model's history\n",
        "\n",
        "def objective(trial):\n",
        "    global best_accuracy, best_history\n",
        "    # Create model with current trial's suggested hyperparameters\n",
        "    model = create_model(trial)\n",
        "\n",
        "    # Setup data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,  # Slightly reduced\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        shear_range=0.1,  # Slightly reduced\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Setup callbacks\n",
        "    checkpoint_filepath = 'best_model_trial_{}.h5'.format(trial.number)\n",
        "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(datagen.flow(input_X_train, output_Y_train, batch_size=128),\n",
        "                        epochs=100,\n",
        "                        validation_data=(input_X_val, output_Y_val),\n",
        "                        callbacks=[checkpoint, early_stopping],\n",
        "                        verbose=1)\n",
        "\n",
        "    current_val_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "    if current_val_accuracy > best_accuracy:  # Check if the current model is the best one\n",
        "        best_accuracy = current_val_accuracy\n",
        "        best_history = history.history  # Update the best_history with the current model's history\n",
        "        print(f\"New best model with validation accuracy: {best_accuracy} saved\")\n",
        "\n",
        "    return current_val_accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=5)  # Adjust the number of trials as needed\n",
        "\n",
        "# Load the best model from the trial that provided the highest validation accuracy\n",
        "best_trial = study.best_trial\n",
        "print('Best trial number:', best_trial.number)\n",
        "best_model_path = 'best_model_trial_{}.h5'.format(best_trial.number)\n",
        "best_model = load_model(best_model_path)\n",
        "\n",
        "# Output the best model's hyperparameters\n",
        "print(\"Best trial's hyperparameters:\")\n",
        "print(f\"Dropout rate: {best_trial.params['dropout_rate']}\")\n",
        "print(f\"Learning rate: {best_trial.params['learning_rate']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgds1IUGMIo2",
        "outputId": "c15d0d3e-b63a-4972-cf38-b3919365d56e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 12:00:36,974] A new study created in memory with name: no-name-016c72d4-1d81-4400-82cc-c8628548e7b2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3fd358b96665>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5859 - accuracy: 0.1877\n",
            "Epoch 1: val_accuracy improved from -inf to 0.33390, saving model to best_model_trial_0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r313/313 [==============================] - 50s 125ms/step - loss: 3.5859 - accuracy: 0.1877 - val_loss: 2.6971 - val_accuracy: 0.3339\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.5298 - accuracy: 0.3656\n",
            "Epoch 2: val_accuracy improved from 0.33390 to 0.40620, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.5298 - accuracy: 0.3656 - val_loss: 2.3348 - val_accuracy: 0.4062\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.1944 - accuracy: 0.4324\n",
            "Epoch 3: val_accuracy improved from 0.40620 to 0.44960, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.1944 - accuracy: 0.4324 - val_loss: 2.1315 - val_accuracy: 0.4496\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.0034 - accuracy: 0.4692\n",
            "Epoch 4: val_accuracy improved from 0.44960 to 0.46780, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 2.0034 - accuracy: 0.4692 - val_loss: 2.0626 - val_accuracy: 0.4678\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.8518 - accuracy: 0.5041\n",
            "Epoch 5: val_accuracy improved from 0.46780 to 0.49910, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.8518 - accuracy: 0.5041 - val_loss: 1.8974 - val_accuracy: 0.4991\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.7486 - accuracy: 0.5315\n",
            "Epoch 6: val_accuracy improved from 0.49910 to 0.52220, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.7486 - accuracy: 0.5315 - val_loss: 1.8066 - val_accuracy: 0.5222\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.6397 - accuracy: 0.5533\n",
            "Epoch 7: val_accuracy did not improve from 0.52220\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.6397 - accuracy: 0.5533 - val_loss: 1.9265 - val_accuracy: 0.5049\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.5634 - accuracy: 0.5721\n",
            "Epoch 8: val_accuracy improved from 0.52220 to 0.52510, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.5634 - accuracy: 0.5721 - val_loss: 1.8082 - val_accuracy: 0.5251\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4894 - accuracy: 0.5861\n",
            "Epoch 9: val_accuracy did not improve from 0.52510\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.4894 - accuracy: 0.5861 - val_loss: 2.1299 - val_accuracy: 0.4765\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4386 - accuracy: 0.6008\n",
            "Epoch 10: val_accuracy improved from 0.52510 to 0.54750, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.4386 - accuracy: 0.6008 - val_loss: 1.7404 - val_accuracy: 0.5475\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.3599 - accuracy: 0.6194\n",
            "Epoch 11: val_accuracy did not improve from 0.54750\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.3599 - accuracy: 0.6194 - val_loss: 1.9445 - val_accuracy: 0.5155\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2996 - accuracy: 0.6353\n",
            "Epoch 12: val_accuracy did not improve from 0.54750\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.2996 - accuracy: 0.6353 - val_loss: 1.8126 - val_accuracy: 0.5394\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2380 - accuracy: 0.6494\n",
            "Epoch 13: val_accuracy did not improve from 0.54750\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.2380 - accuracy: 0.6494 - val_loss: 1.8178 - val_accuracy: 0.5437\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2000 - accuracy: 0.6592\n",
            "Epoch 14: val_accuracy improved from 0.54750 to 0.56050, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 1.2000 - accuracy: 0.6592 - val_loss: 1.7108 - val_accuracy: 0.5605\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1541 - accuracy: 0.6677\n",
            "Epoch 15: val_accuracy did not improve from 0.56050\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.1541 - accuracy: 0.6677 - val_loss: 1.7626 - val_accuracy: 0.5544\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1031 - accuracy: 0.6840\n",
            "Epoch 16: val_accuracy did not improve from 0.56050\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.1031 - accuracy: 0.6840 - val_loss: 1.8183 - val_accuracy: 0.5445\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0487 - accuracy: 0.6983\n",
            "Epoch 17: val_accuracy did not improve from 0.56050\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.0487 - accuracy: 0.6983 - val_loss: 1.8000 - val_accuracy: 0.5532\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0028 - accuracy: 0.7068\n",
            "Epoch 18: val_accuracy did not improve from 0.56050\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.0028 - accuracy: 0.7068 - val_loss: 1.9700 - val_accuracy: 0.5313\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9507 - accuracy: 0.7222\n",
            "Epoch 19: val_accuracy did not improve from 0.56050\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 0.9507 - accuracy: 0.7222 - val_loss: 1.8447 - val_accuracy: 0.5530\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9187 - accuracy: 0.7297\n",
            "Epoch 20: val_accuracy improved from 0.56050 to 0.56510, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 0.9187 - accuracy: 0.7297 - val_loss: 1.8607 - val_accuracy: 0.5651\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.7376\n",
            "Epoch 21: val_accuracy did not improve from 0.56510\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 0.8883 - accuracy: 0.7376 - val_loss: 1.8744 - val_accuracy: 0.5489\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.8458 - accuracy: 0.7504\n",
            "Epoch 22: val_accuracy did not improve from 0.56510\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 0.8458 - accuracy: 0.7504 - val_loss: 1.9677 - val_accuracy: 0.5460\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.7584\n",
            "Epoch 23: val_accuracy improved from 0.56510 to 0.57900, saving model to best_model_trial_0.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 0.8143 - accuracy: 0.7584 - val_loss: 1.7583 - val_accuracy: 0.5790\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.7755 - accuracy: 0.7669\n",
            "Epoch 24: val_accuracy did not improve from 0.57900\n",
            "313/313 [==============================] - 35s 112ms/step - loss: 0.7755 - accuracy: 0.7669 - val_loss: 1.8854 - val_accuracy: 0.5591\n",
            "Epoch 24: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 12:15:16,698] Trial 0 finished with value: 0.5789999961853027 and parameters: {'dropout_rate': 0.40312251007669303, 'learning_rate': 0.0003698119115502871}. Best is trial 0 with value: 0.5789999961853027.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best model with validation accuracy: 0.5789999961853027 saved\n",
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5267 - accuracy: 0.1970\n",
            "Epoch 1: val_accuracy improved from -inf to 0.28480, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 43s 120ms/step - loss: 3.5267 - accuracy: 0.1970 - val_loss: 2.9141 - val_accuracy: 0.2848\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.4958 - accuracy: 0.3656\n",
            "Epoch 2: val_accuracy improved from 0.28480 to 0.39620, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.4958 - accuracy: 0.3656 - val_loss: 2.3310 - val_accuracy: 0.3962\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.1689 - accuracy: 0.4318\n",
            "Epoch 3: val_accuracy improved from 0.39620 to 0.44950, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 36s 117ms/step - loss: 2.1689 - accuracy: 0.4318 - val_loss: 2.1099 - val_accuracy: 0.4495\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.9727 - accuracy: 0.4777\n",
            "Epoch 4: val_accuracy improved from 0.44950 to 0.45830, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.9727 - accuracy: 0.4777 - val_loss: 2.0842 - val_accuracy: 0.4583\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.8421 - accuracy: 0.5032\n",
            "Epoch 5: val_accuracy did not improve from 0.45830\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.8421 - accuracy: 0.5032 - val_loss: 2.1623 - val_accuracy: 0.4563\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.7385 - accuracy: 0.5306\n",
            "Epoch 6: val_accuracy improved from 0.45830 to 0.48660, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.7385 - accuracy: 0.5306 - val_loss: 1.9764 - val_accuracy: 0.4866\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.6378 - accuracy: 0.5498\n",
            "Epoch 7: val_accuracy did not improve from 0.48660\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.6378 - accuracy: 0.5498 - val_loss: 2.0695 - val_accuracy: 0.4729\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.5724\n",
            "Epoch 8: val_accuracy improved from 0.48660 to 0.49770, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 1.5528 - accuracy: 0.5724 - val_loss: 1.9363 - val_accuracy: 0.4977\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4848 - accuracy: 0.5888\n",
            "Epoch 9: val_accuracy improved from 0.49770 to 0.51590, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 1.4848 - accuracy: 0.5888 - val_loss: 1.8926 - val_accuracy: 0.5159\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4175 - accuracy: 0.6020\n",
            "Epoch 10: val_accuracy improved from 0.51590 to 0.54500, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.4175 - accuracy: 0.6020 - val_loss: 1.7495 - val_accuracy: 0.5450\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.3507 - accuracy: 0.6211\n",
            "Epoch 11: val_accuracy did not improve from 0.54500\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.3507 - accuracy: 0.6211 - val_loss: 1.8563 - val_accuracy: 0.5225\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2985 - accuracy: 0.6320\n",
            "Epoch 12: val_accuracy did not improve from 0.54500\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.2985 - accuracy: 0.6320 - val_loss: 1.8256 - val_accuracy: 0.5297\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2344 - accuracy: 0.6472\n",
            "Epoch 13: val_accuracy did not improve from 0.54500\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.2344 - accuracy: 0.6472 - val_loss: 1.8193 - val_accuracy: 0.5380\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1817 - accuracy: 0.6608\n",
            "Epoch 14: val_accuracy did not improve from 0.54500\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 1.1817 - accuracy: 0.6608 - val_loss: 1.8263 - val_accuracy: 0.5383\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1356 - accuracy: 0.6730\n",
            "Epoch 15: val_accuracy improved from 0.54500 to 0.54630, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.1356 - accuracy: 0.6730 - val_loss: 1.8090 - val_accuracy: 0.5463\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0787 - accuracy: 0.6868\n",
            "Epoch 16: val_accuracy did not improve from 0.54630\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.0787 - accuracy: 0.6868 - val_loss: 1.9440 - val_accuracy: 0.5273\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0431 - accuracy: 0.6987\n",
            "Epoch 17: val_accuracy did not improve from 0.54630\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.0431 - accuracy: 0.6987 - val_loss: 1.9929 - val_accuracy: 0.5188\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9819 - accuracy: 0.7118\n",
            "Epoch 18: val_accuracy improved from 0.54630 to 0.54770, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 0.9819 - accuracy: 0.7118 - val_loss: 1.8437 - val_accuracy: 0.5477\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9431 - accuracy: 0.7206\n",
            "Epoch 19: val_accuracy did not improve from 0.54770\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 0.9431 - accuracy: 0.7206 - val_loss: 1.8856 - val_accuracy: 0.5456\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9181 - accuracy: 0.7293\n",
            "Epoch 20: val_accuracy improved from 0.54770 to 0.55150, saving model to best_model_trial_1.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 0.9181 - accuracy: 0.7293 - val_loss: 1.8098 - val_accuracy: 0.5515\n",
            "Epoch 20: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 12:27:26,796] Trial 1 finished with value: 0.5515000224113464 and parameters: {'dropout_rate': 0.3202572538854486, 'learning_rate': 0.00041169421144257774}. Best is trial 0 with value: 0.5789999961853027.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7215 - accuracy: 0.1593\n",
            "Epoch 1: val_accuracy improved from -inf to 0.19360, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 43s 120ms/step - loss: 3.7215 - accuracy: 0.1593 - val_loss: 3.5066 - val_accuracy: 0.1936\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8610 - accuracy: 0.2931\n",
            "Epoch 2: val_accuracy improved from 0.19360 to 0.30080, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 2.8610 - accuracy: 0.2931 - val_loss: 2.9773 - val_accuracy: 0.3008\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.5776 - accuracy: 0.3503\n",
            "Epoch 3: val_accuracy did not improve from 0.30080\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.5776 - accuracy: 0.3503 - val_loss: 3.3072 - val_accuracy: 0.2784\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.4419 - accuracy: 0.3735\n",
            "Epoch 4: val_accuracy improved from 0.30080 to 0.30420, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 2.4419 - accuracy: 0.3735 - val_loss: 2.9298 - val_accuracy: 0.3042\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.3668 - accuracy: 0.3907\n",
            "Epoch 5: val_accuracy improved from 0.30420 to 0.34640, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.3668 - accuracy: 0.3907 - val_loss: 2.7024 - val_accuracy: 0.3464\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.2506 - accuracy: 0.4183\n",
            "Epoch 6: val_accuracy improved from 0.34640 to 0.35330, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.2506 - accuracy: 0.4183 - val_loss: 2.7115 - val_accuracy: 0.3533\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.1202 - accuracy: 0.4421\n",
            "Epoch 7: val_accuracy improved from 0.35330 to 0.41770, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 2.1202 - accuracy: 0.4421 - val_loss: 2.3804 - val_accuracy: 0.4177\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.0091 - accuracy: 0.4636\n",
            "Epoch 8: val_accuracy improved from 0.41770 to 0.46060, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.0091 - accuracy: 0.4636 - val_loss: 2.0632 - val_accuracy: 0.4606\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.9262 - accuracy: 0.4845\n",
            "Epoch 9: val_accuracy did not improve from 0.46060\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.9262 - accuracy: 0.4845 - val_loss: 2.4108 - val_accuracy: 0.4269\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.8611 - accuracy: 0.4976\n",
            "Epoch 10: val_accuracy did not improve from 0.46060\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 1.8611 - accuracy: 0.4976 - val_loss: 2.2345 - val_accuracy: 0.4461\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.7732 - accuracy: 0.5193\n",
            "Epoch 11: val_accuracy improved from 0.46060 to 0.46620, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.7732 - accuracy: 0.5193 - val_loss: 2.1156 - val_accuracy: 0.4662\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.7127 - accuracy: 0.5329\n",
            "Epoch 12: val_accuracy improved from 0.46620 to 0.48060, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 1.7127 - accuracy: 0.5329 - val_loss: 2.0461 - val_accuracy: 0.4806\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.6550 - accuracy: 0.5451\n",
            "Epoch 13: val_accuracy improved from 0.48060 to 0.48450, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 119ms/step - loss: 1.6550 - accuracy: 0.5451 - val_loss: 2.0382 - val_accuracy: 0.4845\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.5985 - accuracy: 0.5579\n",
            "Epoch 14: val_accuracy did not improve from 0.48450\n",
            "313/313 [==============================] - 36s 115ms/step - loss: 1.5985 - accuracy: 0.5579 - val_loss: 2.0678 - val_accuracy: 0.4697\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.5609 - accuracy: 0.5665\n",
            "Epoch 15: val_accuracy improved from 0.48450 to 0.50380, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 1.5609 - accuracy: 0.5665 - val_loss: 1.9156 - val_accuracy: 0.5038\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.5154 - accuracy: 0.5777\n",
            "Epoch 16: val_accuracy improved from 0.50380 to 0.51760, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 119ms/step - loss: 1.5154 - accuracy: 0.5777 - val_loss: 1.8577 - val_accuracy: 0.5176\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4595 - accuracy: 0.5929\n",
            "Epoch 17: val_accuracy improved from 0.51760 to 0.51770, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 1.4595 - accuracy: 0.5929 - val_loss: 1.8693 - val_accuracy: 0.5177\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.4185 - accuracy: 0.5982\n",
            "Epoch 18: val_accuracy improved from 0.51770 to 0.51950, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 1.4185 - accuracy: 0.5982 - val_loss: 1.8891 - val_accuracy: 0.5195\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.3650 - accuracy: 0.6143\n",
            "Epoch 19: val_accuracy did not improve from 0.51950\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.3650 - accuracy: 0.6143 - val_loss: 2.2371 - val_accuracy: 0.4711\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.3384 - accuracy: 0.6177\n",
            "Epoch 20: val_accuracy did not improve from 0.51950\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.3384 - accuracy: 0.6177 - val_loss: 2.0426 - val_accuracy: 0.5051\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2946 - accuracy: 0.6315\n",
            "Epoch 21: val_accuracy did not improve from 0.51950\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.2946 - accuracy: 0.6315 - val_loss: 2.1036 - val_accuracy: 0.4973\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2519 - accuracy: 0.6425\n",
            "Epoch 22: val_accuracy did not improve from 0.51950\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.2519 - accuracy: 0.6425 - val_loss: 2.0950 - val_accuracy: 0.4971\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.2234 - accuracy: 0.6504\n",
            "Epoch 23: val_accuracy improved from 0.51950 to 0.54710, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 1.2234 - accuracy: 0.6504 - val_loss: 1.8247 - val_accuracy: 0.5471\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1811 - accuracy: 0.6617\n",
            "Epoch 24: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.1811 - accuracy: 0.6617 - val_loss: 2.1371 - val_accuracy: 0.5039\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1493 - accuracy: 0.6679\n",
            "Epoch 25: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.1493 - accuracy: 0.6679 - val_loss: 2.3992 - val_accuracy: 0.4772\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.1137 - accuracy: 0.6789\n",
            "Epoch 26: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.1137 - accuracy: 0.6789 - val_loss: 2.2298 - val_accuracy: 0.4977\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0785 - accuracy: 0.6875\n",
            "Epoch 27: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.0785 - accuracy: 0.6875 - val_loss: 1.9203 - val_accuracy: 0.5351\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0358 - accuracy: 0.7001\n",
            "Epoch 28: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 1.0358 - accuracy: 0.7001 - val_loss: 1.9715 - val_accuracy: 0.5297\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 1.0205 - accuracy: 0.6998\n",
            "Epoch 29: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 115ms/step - loss: 1.0205 - accuracy: 0.6998 - val_loss: 2.0455 - val_accuracy: 0.5302\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9895 - accuracy: 0.7097\n",
            "Epoch 30: val_accuracy did not improve from 0.54710\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 0.9895 - accuracy: 0.7097 - val_loss: 2.2521 - val_accuracy: 0.5016\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9504 - accuracy: 0.7222\n",
            "Epoch 31: val_accuracy improved from 0.54710 to 0.55130, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 119ms/step - loss: 0.9504 - accuracy: 0.7222 - val_loss: 1.9516 - val_accuracy: 0.5513\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.9175 - accuracy: 0.7295\n",
            "Epoch 32: val_accuracy did not improve from 0.55130\n",
            "313/313 [==============================] - 36s 115ms/step - loss: 0.9175 - accuracy: 0.7295 - val_loss: 2.3237 - val_accuracy: 0.4974\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.8819 - accuracy: 0.7363\n",
            "Epoch 33: val_accuracy improved from 0.55130 to 0.55500, saving model to best_model_trial_2.h5\n",
            "313/313 [==============================] - 37s 119ms/step - loss: 0.8819 - accuracy: 0.7363 - val_loss: 1.9422 - val_accuracy: 0.5550\n",
            "Epoch 33: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 12:47:34,057] Trial 2 finished with value: 0.5550000071525574 and parameters: {'dropout_rate': 0.4768193816938566, 'learning_rate': 0.0008069054302657859}. Best is trial 0 with value: 0.5789999961853027.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.5287 - accuracy: 0.0213\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01500, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 43s 120ms/step - loss: 4.5287 - accuracy: 0.0213 - val_loss: 5.1018 - val_accuracy: 0.0150\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.2145 - accuracy: 0.0406\n",
            "Epoch 2: val_accuracy improved from 0.01500 to 0.03920, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 4.2145 - accuracy: 0.0406 - val_loss: 4.5858 - val_accuracy: 0.0392\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.1443 - accuracy: 0.0476\n",
            "Epoch 3: val_accuracy did not improve from 0.03920\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 4.1443 - accuracy: 0.0476 - val_loss: 4.4908 - val_accuracy: 0.0315\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.0641 - accuracy: 0.0598\n",
            "Epoch 4: val_accuracy improved from 0.03920 to 0.06140, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 4.0641 - accuracy: 0.0598 - val_loss: 4.1290 - val_accuracy: 0.0614\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9961 - accuracy: 0.0694\n",
            "Epoch 5: val_accuracy improved from 0.06140 to 0.06240, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.9961 - accuracy: 0.0694 - val_loss: 4.1165 - val_accuracy: 0.0624\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9885 - accuracy: 0.0765\n",
            "Epoch 6: val_accuracy did not improve from 0.06240\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.9885 - accuracy: 0.0765 - val_loss: 4.2317 - val_accuracy: 0.0538\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9031 - accuracy: 0.0863\n",
            "Epoch 7: val_accuracy did not improve from 0.06240\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.9031 - accuracy: 0.0863 - val_loss: 5.1164 - val_accuracy: 0.0294\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.0591 - accuracy: 0.0680\n",
            "Epoch 8: val_accuracy did not improve from 0.06240\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 4.0591 - accuracy: 0.0680 - val_loss: 4.2661 - val_accuracy: 0.0610\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.1510 - accuracy: 0.0558\n",
            "Epoch 9: val_accuracy did not improve from 0.06240\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 4.1510 - accuracy: 0.0558 - val_loss: 4.0539 - val_accuracy: 0.0620\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.0287 - accuracy: 0.0710\n",
            "Epoch 10: val_accuracy improved from 0.06240 to 0.09130, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 4.0287 - accuracy: 0.0710 - val_loss: 3.9035 - val_accuracy: 0.0913\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9132 - accuracy: 0.0903\n",
            "Epoch 11: val_accuracy improved from 0.09130 to 0.10490, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.9132 - accuracy: 0.0903 - val_loss: 3.7856 - val_accuracy: 0.1049\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9554 - accuracy: 0.0839\n",
            "Epoch 12: val_accuracy did not improve from 0.10490\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.9554 - accuracy: 0.0839 - val_loss: 5.3936 - val_accuracy: 0.0570\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.9232 - accuracy: 0.0877\n",
            "Epoch 13: val_accuracy improved from 0.10490 to 0.11340, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.9232 - accuracy: 0.0877 - val_loss: 3.7299 - val_accuracy: 0.1134\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.8589 - accuracy: 0.0963\n",
            "Epoch 14: val_accuracy improved from 0.11340 to 0.11740, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.8589 - accuracy: 0.0963 - val_loss: 3.7673 - val_accuracy: 0.1174\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7800 - accuracy: 0.1067\n",
            "Epoch 15: val_accuracy improved from 0.11740 to 0.12470, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.7800 - accuracy: 0.1067 - val_loss: 3.6549 - val_accuracy: 0.1247\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7779 - accuracy: 0.1096\n",
            "Epoch 16: val_accuracy did not improve from 0.12470\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.7779 - accuracy: 0.1096 - val_loss: 3.7158 - val_accuracy: 0.1174\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.8441 - accuracy: 0.0990\n",
            "Epoch 17: val_accuracy did not improve from 0.12470\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.8441 - accuracy: 0.0990 - val_loss: 3.8993 - val_accuracy: 0.0922\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7817 - accuracy: 0.1113\n",
            "Epoch 18: val_accuracy improved from 0.12470 to 0.12540, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 3.7817 - accuracy: 0.1113 - val_loss: 3.6753 - val_accuracy: 0.1254\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7383 - accuracy: 0.1167\n",
            "Epoch 19: val_accuracy improved from 0.12540 to 0.13440, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.7383 - accuracy: 0.1167 - val_loss: 3.6182 - val_accuracy: 0.1344\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7073 - accuracy: 0.1202\n",
            "Epoch 20: val_accuracy improved from 0.13440 to 0.13750, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.7073 - accuracy: 0.1202 - val_loss: 3.6137 - val_accuracy: 0.1375\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6709 - accuracy: 0.1279\n",
            "Epoch 21: val_accuracy improved from 0.13750 to 0.15020, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.6709 - accuracy: 0.1279 - val_loss: 3.5132 - val_accuracy: 0.1502\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6472 - accuracy: 0.1298\n",
            "Epoch 22: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6472 - accuracy: 0.1298 - val_loss: 3.6954 - val_accuracy: 0.1201\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6158 - accuracy: 0.1391\n",
            "Epoch 23: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6158 - accuracy: 0.1391 - val_loss: 4.3225 - val_accuracy: 0.0602\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7972 - accuracy: 0.1164\n",
            "Epoch 24: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7972 - accuracy: 0.1164 - val_loss: 12.2336 - val_accuracy: 0.0394\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7610 - accuracy: 0.1141\n",
            "Epoch 25: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7610 - accuracy: 0.1141 - val_loss: 3.5752 - val_accuracy: 0.1415\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6288 - accuracy: 0.1342\n",
            "Epoch 26: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6288 - accuracy: 0.1342 - val_loss: 4.2177 - val_accuracy: 0.0838\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7978 - accuracy: 0.1084\n",
            "Epoch 27: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7978 - accuracy: 0.1084 - val_loss: 3.7359 - val_accuracy: 0.1145\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6768 - accuracy: 0.1251\n",
            "Epoch 28: val_accuracy did not improve from 0.15020\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6768 - accuracy: 0.1251 - val_loss: 3.8278 - val_accuracy: 0.1260\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6423 - accuracy: 0.1320\n",
            "Epoch 29: val_accuracy improved from 0.15020 to 0.16210, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.6423 - accuracy: 0.1320 - val_loss: 3.4593 - val_accuracy: 0.1621\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6024 - accuracy: 0.1363\n",
            "Epoch 30: val_accuracy improved from 0.16210 to 0.16370, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.6024 - accuracy: 0.1363 - val_loss: 3.4222 - val_accuracy: 0.1637\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5530 - accuracy: 0.1468\n",
            "Epoch 31: val_accuracy did not improve from 0.16370\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.5530 - accuracy: 0.1468 - val_loss: 3.4663 - val_accuracy: 0.1608\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5209 - accuracy: 0.1521\n",
            "Epoch 32: val_accuracy improved from 0.16370 to 0.17170, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.5209 - accuracy: 0.1521 - val_loss: 3.4235 - val_accuracy: 0.1717\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5250 - accuracy: 0.1552\n",
            "Epoch 33: val_accuracy did not improve from 0.17170\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.5250 - accuracy: 0.1552 - val_loss: 3.4413 - val_accuracy: 0.1678\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4643 - accuracy: 0.1652\n",
            "Epoch 34: val_accuracy improved from 0.17170 to 0.19320, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.4643 - accuracy: 0.1652 - val_loss: 3.2958 - val_accuracy: 0.1932\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4032 - accuracy: 0.1707\n",
            "Epoch 35: val_accuracy improved from 0.19320 to 0.21230, saving model to best_model_trial_3.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.4032 - accuracy: 0.1707 - val_loss: 3.2291 - val_accuracy: 0.2123\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4281 - accuracy: 0.1698\n",
            "Epoch 36: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.4281 - accuracy: 0.1698 - val_loss: 3.2696 - val_accuracy: 0.1999\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5108 - accuracy: 0.1565\n",
            "Epoch 37: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.5108 - accuracy: 0.1565 - val_loss: 3.9800 - val_accuracy: 0.1172\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4796 - accuracy: 0.1596\n",
            "Epoch 38: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.4796 - accuracy: 0.1596 - val_loss: 3.3701 - val_accuracy: 0.1797\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.3997 - accuracy: 0.1735\n",
            "Epoch 39: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.3997 - accuracy: 0.1735 - val_loss: 3.2676 - val_accuracy: 0.1988\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4294 - accuracy: 0.1697\n",
            "Epoch 40: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.4294 - accuracy: 0.1697 - val_loss: 3.2017 - val_accuracy: 0.2089\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4014 - accuracy: 0.1736\n",
            "Epoch 41: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.4014 - accuracy: 0.1736 - val_loss: 3.2479 - val_accuracy: 0.1991\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7874 - accuracy: 0.1240\n",
            "Epoch 42: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7874 - accuracy: 0.1240 - val_loss: 3.3919 - val_accuracy: 0.1803\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4975 - accuracy: 0.1548\n",
            "Epoch 43: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.4975 - accuracy: 0.1548 - val_loss: 3.3624 - val_accuracy: 0.1808\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.8211 - accuracy: 0.1105\n",
            "Epoch 44: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.8211 - accuracy: 0.1105 - val_loss: 3.5490 - val_accuracy: 0.1463\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6585 - accuracy: 0.1363\n",
            "Epoch 45: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6585 - accuracy: 0.1363 - val_loss: 3.9488 - val_accuracy: 0.0940\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6272 - accuracy: 0.1407\n",
            "Epoch 46: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.6272 - accuracy: 0.1407 - val_loss: 3.5222 - val_accuracy: 0.1599\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5058 - accuracy: 0.1573\n",
            "Epoch 47: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.5058 - accuracy: 0.1573 - val_loss: 3.3307 - val_accuracy: 0.1857\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4727 - accuracy: 0.1648\n",
            "Epoch 48: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 112ms/step - loss: 3.4727 - accuracy: 0.1648 - val_loss: 3.3705 - val_accuracy: 0.1831\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4457 - accuracy: 0.1647\n",
            "Epoch 49: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.4457 - accuracy: 0.1647 - val_loss: 3.2802 - val_accuracy: 0.1979\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4703 - accuracy: 0.1622\n",
            "Epoch 50: val_accuracy did not improve from 0.21230\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.4703 - accuracy: 0.1622 - val_loss: 3.3298 - val_accuracy: 0.1833\n",
            "Epoch 50: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 13:17:38,975] Trial 3 finished with value: 0.21230000257492065 and parameters: {'dropout_rate': 0.31884803848208115, 'learning_rate': 0.007816480836290916}. Best is trial 0 with value: 0.5789999961853027.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.4200 - accuracy: 0.0337\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01870, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 44s 120ms/step - loss: 4.4200 - accuracy: 0.0337 - val_loss: 4.7968 - val_accuracy: 0.0187\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 4.0294 - accuracy: 0.0667\n",
            "Epoch 2: val_accuracy did not improve from 0.01870\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 4.0294 - accuracy: 0.0667 - val_loss: 4.8480 - val_accuracy: 0.0184\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.8461 - accuracy: 0.0966\n",
            "Epoch 3: val_accuracy improved from 0.01870 to 0.06380, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.8461 - accuracy: 0.0966 - val_loss: 4.5566 - val_accuracy: 0.0638\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7886 - accuracy: 0.1075\n",
            "Epoch 4: val_accuracy did not improve from 0.06380\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7886 - accuracy: 0.1075 - val_loss: 7.5016 - val_accuracy: 0.0329\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.8413 - accuracy: 0.1011\n",
            "Epoch 5: val_accuracy improved from 0.06380 to 0.09330, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 3.8413 - accuracy: 0.1011 - val_loss: 3.9762 - val_accuracy: 0.0933\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.7088 - accuracy: 0.1206\n",
            "Epoch 6: val_accuracy did not improve from 0.09330\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.7088 - accuracy: 0.1206 - val_loss: 4.5998 - val_accuracy: 0.0607\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5955 - accuracy: 0.1388\n",
            "Epoch 7: val_accuracy improved from 0.09330 to 0.11820, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.5955 - accuracy: 0.1388 - val_loss: 3.9655 - val_accuracy: 0.1182\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5954 - accuracy: 0.1407\n",
            "Epoch 8: val_accuracy did not improve from 0.11820\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.5954 - accuracy: 0.1407 - val_loss: 4.9839 - val_accuracy: 0.0738\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.6252 - accuracy: 0.1365\n",
            "Epoch 9: val_accuracy improved from 0.11820 to 0.12190, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 3.6252 - accuracy: 0.1365 - val_loss: 3.9393 - val_accuracy: 0.1219\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5186 - accuracy: 0.1556\n",
            "Epoch 10: val_accuracy did not improve from 0.12190\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.5186 - accuracy: 0.1556 - val_loss: 4.7971 - val_accuracy: 0.0681\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5403 - accuracy: 0.1527\n",
            "Epoch 11: val_accuracy improved from 0.12190 to 0.13950, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.5403 - accuracy: 0.1527 - val_loss: 3.5773 - val_accuracy: 0.1395\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.5722 - accuracy: 0.1499\n",
            "Epoch 12: val_accuracy did not improve from 0.13950\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.5722 - accuracy: 0.1499 - val_loss: 3.9828 - val_accuracy: 0.1031\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4712 - accuracy: 0.1632\n",
            "Epoch 13: val_accuracy did not improve from 0.13950\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.4712 - accuracy: 0.1632 - val_loss: 4.0961 - val_accuracy: 0.1330\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4686 - accuracy: 0.1667\n",
            "Epoch 14: val_accuracy improved from 0.13950 to 0.14430, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.4686 - accuracy: 0.1667 - val_loss: 4.6577 - val_accuracy: 0.1443\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.4057 - accuracy: 0.1783\n",
            "Epoch 15: val_accuracy improved from 0.14430 to 0.16430, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.4057 - accuracy: 0.1783 - val_loss: 3.6020 - val_accuracy: 0.1643\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.3912 - accuracy: 0.1760\n",
            "Epoch 16: val_accuracy did not improve from 0.16430\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.3912 - accuracy: 0.1760 - val_loss: 4.1539 - val_accuracy: 0.0882\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.3943 - accuracy: 0.1759\n",
            "Epoch 17: val_accuracy did not improve from 0.16430\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.3943 - accuracy: 0.1759 - val_loss: 3.5991 - val_accuracy: 0.1507\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2963 - accuracy: 0.1936\n",
            "Epoch 18: val_accuracy improved from 0.16430 to 0.18540, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.2963 - accuracy: 0.1936 - val_loss: 3.4095 - val_accuracy: 0.1854\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2325 - accuracy: 0.2056\n",
            "Epoch 19: val_accuracy improved from 0.18540 to 0.21740, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.2325 - accuracy: 0.2056 - val_loss: 3.1965 - val_accuracy: 0.2174\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.3438 - accuracy: 0.1891\n",
            "Epoch 20: val_accuracy did not improve from 0.21740\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.3438 - accuracy: 0.1891 - val_loss: 4.8516 - val_accuracy: 0.0932\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2942 - accuracy: 0.1931\n",
            "Epoch 21: val_accuracy did not improve from 0.21740\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.2942 - accuracy: 0.1931 - val_loss: 4.9227 - val_accuracy: 0.0890\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2579 - accuracy: 0.1997\n",
            "Epoch 22: val_accuracy did not improve from 0.21740\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.2579 - accuracy: 0.1997 - val_loss: 3.3236 - val_accuracy: 0.2106\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2170 - accuracy: 0.2119\n",
            "Epoch 23: val_accuracy did not improve from 0.21740\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.2170 - accuracy: 0.2119 - val_loss: 3.7168 - val_accuracy: 0.1981\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2012 - accuracy: 0.2153\n",
            "Epoch 24: val_accuracy improved from 0.21740 to 0.23100, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 3.2012 - accuracy: 0.2153 - val_loss: 3.3898 - val_accuracy: 0.2310\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1504 - accuracy: 0.2207\n",
            "Epoch 25: val_accuracy did not improve from 0.23100\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.1504 - accuracy: 0.2207 - val_loss: 3.3228 - val_accuracy: 0.1950\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2263 - accuracy: 0.2063\n",
            "Epoch 26: val_accuracy did not improve from 0.23100\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.2263 - accuracy: 0.2063 - val_loss: 3.3891 - val_accuracy: 0.1937\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0926 - accuracy: 0.2283\n",
            "Epoch 27: val_accuracy improved from 0.23100 to 0.24260, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 3.0926 - accuracy: 0.2283 - val_loss: 3.1427 - val_accuracy: 0.2426\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0736 - accuracy: 0.2351\n",
            "Epoch 28: val_accuracy improved from 0.24260 to 0.24470, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 118ms/step - loss: 3.0736 - accuracy: 0.2351 - val_loss: 3.0851 - val_accuracy: 0.2447\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2546 - accuracy: 0.2043\n",
            "Epoch 29: val_accuracy did not improve from 0.24470\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.2546 - accuracy: 0.2043 - val_loss: 3.5675 - val_accuracy: 0.1638\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2717 - accuracy: 0.2025\n",
            "Epoch 30: val_accuracy did not improve from 0.24470\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.2717 - accuracy: 0.2025 - val_loss: 4.0507 - val_accuracy: 0.1496\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1548 - accuracy: 0.2212\n",
            "Epoch 31: val_accuracy improved from 0.24470 to 0.25870, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 3.1548 - accuracy: 0.2212 - val_loss: 2.9341 - val_accuracy: 0.2587\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0930 - accuracy: 0.2326\n",
            "Epoch 32: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.0930 - accuracy: 0.2326 - val_loss: 3.1406 - val_accuracy: 0.2381\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.2881 - accuracy: 0.2031\n",
            "Epoch 33: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.2881 - accuracy: 0.2031 - val_loss: 3.2241 - val_accuracy: 0.2223\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1704 - accuracy: 0.2154\n",
            "Epoch 34: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.1704 - accuracy: 0.2154 - val_loss: 3.7491 - val_accuracy: 0.1376\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1307 - accuracy: 0.2241\n",
            "Epoch 35: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.1307 - accuracy: 0.2241 - val_loss: 3.0939 - val_accuracy: 0.2524\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1018 - accuracy: 0.2298\n",
            "Epoch 36: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.1018 - accuracy: 0.2298 - val_loss: 3.0927 - val_accuracy: 0.2383\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0500 - accuracy: 0.2405\n",
            "Epoch 37: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.0500 - accuracy: 0.2405 - val_loss: 3.2942 - val_accuracy: 0.2075\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9923 - accuracy: 0.2524\n",
            "Epoch 38: val_accuracy did not improve from 0.25870\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 2.9923 - accuracy: 0.2524 - val_loss: 3.5702 - val_accuracy: 0.1823\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9375 - accuracy: 0.2590\n",
            "Epoch 39: val_accuracy improved from 0.25870 to 0.28930, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.9375 - accuracy: 0.2590 - val_loss: 2.7953 - val_accuracy: 0.2893\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9536 - accuracy: 0.2578\n",
            "Epoch 40: val_accuracy did not improve from 0.28930\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.9536 - accuracy: 0.2578 - val_loss: 2.9426 - val_accuracy: 0.2595\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0311 - accuracy: 0.2454\n",
            "Epoch 41: val_accuracy did not improve from 0.28930\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.0311 - accuracy: 0.2454 - val_loss: 3.8529 - val_accuracy: 0.1342\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0885 - accuracy: 0.2385\n",
            "Epoch 42: val_accuracy did not improve from 0.28930\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.0885 - accuracy: 0.2385 - val_loss: 2.9901 - val_accuracy: 0.2599\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8834 - accuracy: 0.2711\n",
            "Epoch 43: val_accuracy improved from 0.28930 to 0.29420, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.8834 - accuracy: 0.2711 - val_loss: 2.7567 - val_accuracy: 0.2942\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8709 - accuracy: 0.2767\n",
            "Epoch 44: val_accuracy did not improve from 0.29420\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8709 - accuracy: 0.2767 - val_loss: 2.9010 - val_accuracy: 0.2734\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9436 - accuracy: 0.2610\n",
            "Epoch 45: val_accuracy did not improve from 0.29420\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.9436 - accuracy: 0.2610 - val_loss: 3.0483 - val_accuracy: 0.2605\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8530 - accuracy: 0.2786\n",
            "Epoch 46: val_accuracy did not improve from 0.29420\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8530 - accuracy: 0.2786 - val_loss: 3.3877 - val_accuracy: 0.2114\n",
            "Epoch 47/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8606 - accuracy: 0.2771\n",
            "Epoch 47: val_accuracy improved from 0.29420 to 0.31160, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.8606 - accuracy: 0.2771 - val_loss: 2.7687 - val_accuracy: 0.3116\n",
            "Epoch 48/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8262 - accuracy: 0.2838\n",
            "Epoch 48: val_accuracy improved from 0.31160 to 0.31580, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.8262 - accuracy: 0.2838 - val_loss: 2.6821 - val_accuracy: 0.3158\n",
            "Epoch 49/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0382 - accuracy: 0.2468\n",
            "Epoch 49: val_accuracy did not improve from 0.31580\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 3.0382 - accuracy: 0.2468 - val_loss: 3.2531 - val_accuracy: 0.2056\n",
            "Epoch 50/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9748 - accuracy: 0.2575\n",
            "Epoch 50: val_accuracy did not improve from 0.31580\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.9748 - accuracy: 0.2575 - val_loss: 2.9878 - val_accuracy: 0.2585\n",
            "Epoch 51/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9223 - accuracy: 0.2693\n",
            "Epoch 51: val_accuracy did not improve from 0.31580\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 2.9223 - accuracy: 0.2693 - val_loss: 2.9029 - val_accuracy: 0.2768\n",
            "Epoch 52/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8790 - accuracy: 0.2756\n",
            "Epoch 52: val_accuracy did not improve from 0.31580\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8790 - accuracy: 0.2756 - val_loss: 3.3285 - val_accuracy: 0.2163\n",
            "Epoch 53/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8211 - accuracy: 0.2878\n",
            "Epoch 53: val_accuracy did not improve from 0.31580\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8211 - accuracy: 0.2878 - val_loss: 2.7387 - val_accuracy: 0.3092\n",
            "Epoch 54/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.7664 - accuracy: 0.2966\n",
            "Epoch 54: val_accuracy improved from 0.31580 to 0.32130, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.7664 - accuracy: 0.2966 - val_loss: 2.6308 - val_accuracy: 0.3213\n",
            "Epoch 55/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1966 - accuracy: 0.2201\n",
            "Epoch 55: val_accuracy did not improve from 0.32130\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.1966 - accuracy: 0.2201 - val_loss: 3.4645 - val_accuracy: 0.2030\n",
            "Epoch 56/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0575 - accuracy: 0.2424\n",
            "Epoch 56: val_accuracy did not improve from 0.32130\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.0575 - accuracy: 0.2424 - val_loss: 2.8305 - val_accuracy: 0.2904\n",
            "Epoch 57/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8772 - accuracy: 0.2756\n",
            "Epoch 57: val_accuracy did not improve from 0.32130\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8772 - accuracy: 0.2756 - val_loss: 2.7484 - val_accuracy: 0.3047\n",
            "Epoch 58/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.7971 - accuracy: 0.2894\n",
            "Epoch 58: val_accuracy did not improve from 0.32130\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.7971 - accuracy: 0.2894 - val_loss: 2.6636 - val_accuracy: 0.3175\n",
            "Epoch 59/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8139 - accuracy: 0.2852\n",
            "Epoch 59: val_accuracy did not improve from 0.32130\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 2.8139 - accuracy: 0.2852 - val_loss: 3.1012 - val_accuracy: 0.2508\n",
            "Epoch 60/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.7465 - accuracy: 0.2987\n",
            "Epoch 60: val_accuracy improved from 0.32130 to 0.32380, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.7465 - accuracy: 0.2987 - val_loss: 2.6309 - val_accuracy: 0.3238\n",
            "Epoch 61/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.6765 - accuracy: 0.3153\n",
            "Epoch 61: val_accuracy improved from 0.32380 to 0.34100, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.6765 - accuracy: 0.3153 - val_loss: 2.5806 - val_accuracy: 0.3410\n",
            "Epoch 62/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.6496 - accuracy: 0.3181\n",
            "Epoch 62: val_accuracy did not improve from 0.34100\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 2.6496 - accuracy: 0.3181 - val_loss: 2.5730 - val_accuracy: 0.3404\n",
            "Epoch 63/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.6331 - accuracy: 0.3224\n",
            "Epoch 63: val_accuracy did not improve from 0.34100\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 2.6331 - accuracy: 0.3224 - val_loss: 2.8893 - val_accuracy: 0.2790\n",
            "Epoch 64/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.6088 - accuracy: 0.3272\n",
            "Epoch 64: val_accuracy improved from 0.34100 to 0.35770, saving model to best_model_trial_4.h5\n",
            "313/313 [==============================] - 37s 117ms/step - loss: 2.6088 - accuracy: 0.3272 - val_loss: 2.4963 - val_accuracy: 0.3577\n",
            "Epoch 65/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9208 - accuracy: 0.2743\n",
            "Epoch 65: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 2.9208 - accuracy: 0.2743 - val_loss: 5.1361 - val_accuracy: 0.0943\n",
            "Epoch 66/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1863 - accuracy: 0.2170\n",
            "Epoch 66: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.1863 - accuracy: 0.2170 - val_loss: 2.9032 - val_accuracy: 0.2804\n",
            "Epoch 67/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9406 - accuracy: 0.2643\n",
            "Epoch 67: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.9406 - accuracy: 0.2643 - val_loss: 2.8127 - val_accuracy: 0.2956\n",
            "Epoch 68/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8342 - accuracy: 0.2846\n",
            "Epoch 68: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8342 - accuracy: 0.2846 - val_loss: 2.6799 - val_accuracy: 0.3221\n",
            "Epoch 69/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.7612 - accuracy: 0.2976\n",
            "Epoch 69: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.7612 - accuracy: 0.2976 - val_loss: 2.6601 - val_accuracy: 0.3256\n",
            "Epoch 70/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.7001 - accuracy: 0.3094\n",
            "Epoch 70: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 36s 114ms/step - loss: 2.7001 - accuracy: 0.3094 - val_loss: 2.6482 - val_accuracy: 0.3279\n",
            "Epoch 71/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.0608 - accuracy: 0.2522\n",
            "Epoch 71: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 3.0608 - accuracy: 0.2522 - val_loss: 3.4917 - val_accuracy: 0.2033\n",
            "Epoch 72/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 3.1376 - accuracy: 0.2280\n",
            "Epoch 72: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 36s 113ms/step - loss: 3.1376 - accuracy: 0.2280 - val_loss: 2.8425 - val_accuracy: 0.2887\n",
            "Epoch 73/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.9406 - accuracy: 0.2661\n",
            "Epoch 73: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.9406 - accuracy: 0.2661 - val_loss: 2.8047 - val_accuracy: 0.2979\n",
            "Epoch 74/100\n",
            "313/313 [==============================] - ETA: 0s - loss: 2.8456 - accuracy: 0.2839\n",
            "Epoch 74: val_accuracy did not improve from 0.35770\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 2.8456 - accuracy: 0.2839 - val_loss: 2.7304 - val_accuracy: 0.3131\n",
            "Epoch 74: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-25 14:02:00,564] Trial 4 finished with value: 0.357699990272522 and parameters: {'dropout_rate': 0.3717953247643471, 'learning_rate': 0.005138053549209233}. Best is trial 0 with value: 0.5789999961853027.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial number: 0\n",
            "Best trial's hyperparameters:\n",
            "Dropout rate: 0.40312251007669303\n",
            "Learning rate: 0.0003698119115502871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(input_X_test, output_Y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFskvJwOMcut",
        "outputId": "7a301856-0c60-4aa0-b339-862001881930"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 6ms/step - loss: 1.7305 - accuracy: 0.5813\n",
            "Test Loss: 1.731\n",
            "Test Accuracy: 0.581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz-bVc5iWNd5",
        "outputId": "f8356b6f-3c49-42a6-9a53-cb3215f5a86b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 38, 38, 3)            0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 16, 16, 64)           9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 16, 16, 64)           256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 16, 16, 64)           0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 18, 18, 64)           0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 8, 8, 64)             0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 8, 8, 64)             4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 8, 8, 64)             36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 8, 8, 256)            16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 8, 8, 256)            16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 8, 8, 256)            1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 8, 8, 256)            1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 8, 8, 256)            0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 8, 8, 256)            0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 8, 8, 64)             16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 8, 8, 64)             36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 8, 8, 256)            16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 8, 8, 256)            1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 8, 8, 256)            0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 8, 8, 256)            0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 8, 8, 64)             16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 8, 8, 64)             36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 8, 8, 64)             256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 8, 8, 64)             0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 8, 8, 256)            16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 8, 8, 256)            1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 8, 8, 256)            0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 8, 8, 256)            0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 4, 4, 128)            32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 4, 4, 128)            147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 4, 4, 512)            131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 4, 4, 512)            66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 4, 4, 512)            2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 4, 4, 512)            2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 4, 4, 512)            0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 4, 4, 512)            0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 4, 4, 128)            65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 4, 4, 128)            147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 4, 4, 512)            66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 4, 4, 512)            2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 4, 4, 512)            0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 4, 4, 512)            0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 4, 4, 128)            65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 4, 4, 128)            147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 4, 4, 512)            66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 4, 4, 512)            2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 4, 4, 512)            0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 4, 4, 512)            0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 4, 4, 128)            65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 4, 4, 128)            147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 4, 4, 128)            512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 4, 4, 128)            0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 4, 4, 512)            66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 4, 4, 512)            2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 4, 4, 512)            0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 4, 4, 512)            0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 2, 2, 256)            131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 2, 2, 1024)           525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 2, 2, 256)            262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 2, 2, 256)            262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 2, 2, 256)            262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 2, 2, 256)            262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 2, 2, 256)            262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 2, 2, 256)            590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 2, 2, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 2, 2, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 2, 2, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 2, 2, 1024)           4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 2, 2, 1024)           0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 2, 2, 1024)           0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 1, 1, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 1, 1, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 1, 1, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 1, 1, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 1, 1, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 1, 1, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 1, 1, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 1, 1, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 1, 1, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 1, 1, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 1, 1, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 1, 1, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 1, 1, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 1, 1, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 1, 1, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 1, 1, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 1, 1, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 1, 1, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 1, 1, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 1, 1, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 1, 1, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 1, 1, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 2048)                 0         ['conv5_block3_out[0][0]']    \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  262144    ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2048)                 262144    ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 1, 1, 2048)           0         ['conv5_block3_out[0][0]',    \n",
            "                                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 2048)                 0         ['multiply[0][0]']            \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 512)                  1049088   ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 512)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 100)                  51300     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25212388 (96.18 MB)\n",
            "Trainable params: 25159268 (95.97 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# Re-compile the best model with Adam optimizer and additional metrics\n",
        "best_model.compile(\n",
        "    optimizer=Adam(lr=best_trial.params['learning_rate']),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = best_model.evaluate(input_X_test, output_Y_test, verbose=1)\n",
        "\n",
        "# Extract metrics\n",
        "test_loss, test_accuracy, test_precision, test_recall = results\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "\n",
        "# Calculate and print F1 Score\n",
        "if (test_precision + test_recall) == 0:\n",
        "    print(\"Precision and recall are both zero; F1 score is undefined.\")\n",
        "else:\n",
        "    test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "    print(f\"Test F1 Score: {test_f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMTUg0oVo5-n",
        "outputId": "1bbb8727-0663-4809-bca5-1966c3f65d0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 7ms/step - loss: 1.7305 - accuracy: 0.5813 - precision: 0.6854 - recall: 0.5296\n",
            "Test Loss: 1.7305\n",
            "Test Accuracy: 0.5813\n",
            "Test Precision: 0.6854\n",
            "Test Recall: 0.5296\n",
            "Test F1 Score: 0.5975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "d1LYorYI81IR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(best_history['loss'], label='Training Loss')\n",
        "plt.plot(best_history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "1Y2dKcqP8yHT",
        "outputId": "58e8ce84-3d41-433b-9066-043279c2e5cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(best_history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(best_history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Dq7XWTSz8zNC",
        "outputId": "960d21c1-d524-4506-c42c-bd6a9ec15d98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}